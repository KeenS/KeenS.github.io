<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
    <head>
        
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

        <meta name="twitter:card" content="summary" />
        <meta name="twitter:site" content="@blackenedgold" />
        <meta name="twitter:title" content="JITあれこれ | κeenのHappy Hacκing Blog"/>
        <meta name="twitter:description" content="κeenです。遅刻してしまいましたがこのエントリーは 言語実装 Advent Calendar 2018 1日目の記事です。
最近私の観測範囲内でJITが流行っているのですが一口にJITと言っても色々あるよなーと思ったので私がJITについて知っていることをグダクダ話します。
" />

        <title>JITあれこれ | κeenのHappy Hacκing Blog</title>
        <link rel="stylesheet" href='//fonts.googleapis.com/css?family=Open+Sans:400,300,600' type='text/css' />
        <link rel="stylesheet" href="//KeenS.github.io/libraries/normalize.3.0.1.css" />
        <link rel="stylesheet" href="//KeenS.github.io/css/liquorice.css" />
        <link rel="apple-touch-icon" sizes="180x180" href="//KeenS.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" href="//KeenS.github.io/favicon-32x32.png" sizes="32x32">
        <link rel="icon" type="image/png" href="//KeenS.github.io/favicon-16x16.png" sizes="16x16">
        <link rel="manifest" href="//KeenS.github.io/manifest.json">
        <link rel="mask-icon" href="//KeenS.github.io/safari-pinned-tab.svg" color="#5bbad5">
        <meta name="theme-color" content="#ffffff">
        <link rel="apple-touch-icon-precomposed" href="//KeenS.github.io/apple-touch-icon-144-precomposed.png" sizes="144x144" />
        <link rel="alternate" href="" type="application/rss+xml" title="κeenのHappy Hacκing Blog" />
        
        <script>
         (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                                  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

         ga('create', 'UA-43779888-1', 'auto');
         ga('send', 'pageview');

        </script>
        
        <link rel="stylesheet" href="//KeenS.github.io/highlight.js/styles/monokai.css" />
<script src="//KeenS.github.io/highlight.js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] } });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
</script>
<meta http-equiv="X-UA-Compatible" CONTENT="IE=EmulateIE7" />
<link rel="stylesheet" href="//unpkg.com/mermaid@7.0.4/dist/mermaid.forest.min.css">

    </head>
    <body class="li-body">

<header class="li-page-header" id="page-header">
    <div class="container">
        <div class="row">
            <div class="sixteen columns">
                <div class="li-brand li-left">
                <a href="//KeenS.github.io/">κeenのHappy Hacκing Blog</a> | Lispエイリアンの狂想曲</div>
                <div class="li-menu li-right">
                    <span class="li-menu-icon" onclick="javascript:toggle('menu');">&#9776;</span>
                    <ul id="menu2" class="li-menu-items">
                        
                            <li><a href="//KeenS.github.io/about/"> About </a></li>
                        
                            <li><a href="//KeenS.github.io/index.xml"> Atom </a></li>
                        
                            <li><a href="//KeenS.github.io/post/"> Blog </a></li>
                        
                            <li><a href="//KeenS.github.io/slide/"> Slide </a></li>
                        
                    </ul>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="sixteen columns">
                <ul id="menu" class="li-menu-items li-menu-mobile">
                    
                        <li><a href="//KeenS.github.io/about/"> About </a></li>
                    
                        <li><a href="//KeenS.github.io/index.xml"> Atom </a></li>
                    
                        <li><a href="//KeenS.github.io/post/"> Blog </a></li>
                    
                        <li><a href="//KeenS.github.io/slide/"> Slide </a></li>
                    
                </ul>
            </div>
        </div>
    </div>
</header>






    <div class="container">
        <div class="row">
            <div class="sixteen columns">
                <article class="li-article">
                    <header class="li-article-header">
                        <h1 class="li-article-title">JITあれこれ</h1>
                        <div class="li-article-meta">
    <time class="li-article-date">2018-12-01</time>
    <ul class="li-article-tag">
    
        <li>
            <a href="//KeenS.github.io/categories/%e8%a8%80%e8%aa%9e%e5%87%a6%e7%90%86%e7%b3%bb">言語処理系</a>
        </li>
    
        <li>
            <a href="//KeenS.github.io/categories/%e8%a8%80%e8%aa%9e%e5%ae%9f%e8%a3%85-adevnt-calendar">言語実装 Adevnt Calendar</a>
        </li>
    
        <li>
            <a href="//KeenS.github.io/categories/%e8%a8%80%e8%aa%9e%e5%ae%9f%e8%a3%85-adevnt-calendar-2018">言語実装 Adevnt Calendar 2018</a>
        </li>
    
        <li>
            <a href="//KeenS.github.io/categories/adevnt-calendar">Adevnt Calendar</a>
        </li>
    
        <li>
            <a href="//KeenS.github.io/categories/adevnt-calendar-2018">Adevnt Calendar 2018</a>
        </li>
    
</ul>

</div>

                    </header>
                    <section>
                        <p>κeenです。遅刻してしまいましたがこのエントリーは <a href="https://qiita.com/advent-calendar/2018/lang_dev">言語実装 Advent Calendar 2018</a> 1日目の記事です。
最近私の観測範囲内でJITが流行っているのですが一口にJITと言っても色々あるよなーと思ったので私がJITについて知っていることをグダクダ話します。
</p>

<p>このブログでも何度がJITや周辺技術について取り上げてますが話の流れがスムーズになるので最初から説明していきます。</p>

<p>2018-12-03: 加筆修正しました。差分は<a href="https://github.com/KeenS/KeenS.github.io/commit/55eea979857879be6f09ef7c52836c29ebedb725#diff-4ca9c4059b3670fab6b5d3a3e1eecc36">こちら</a></p>

<h1 id="jitって">JITって？</h1>

<p>Just in Time(コンパイル)のことで、日本語にすると「間に合ってコンパイル」になりますかね。
インタプリタの高速化テクニックの1つです。
最初はインタプリタのようにコードをコンパイルせずプロセスが起動しますが、メソッドを実行するまでにはメソッドをコンパイルして、ネイティブコードで実行する方式です。</p>

<p>本来ならJITはこのような意味なのですがここではもう少し範囲を広めに取って実行プロセス起動後にコードを生成して実行するものを全部JITと呼ぶことにします。
たとえばCommon Lispの実装の1つ、SBCLは関数を定義するときにネイティブコンパイルするのでJITとして扱うことにします。</p>

<p>余談ですが動的型付言語がネイティブコンパイルできることに驚く方もいますが動的型付言語のインタプリタもCで書かれてコンパイルされているのでコンパイルできない道理はないです。
「動的に型チェックするコード」を生成すればいいだけです。</p>

<h1 id="コンパイルのタイミング">コンパイルのタイミング</h1>

<p>プロセスが起動してから実行するまでにコンパイルすればいいのでコンパイルするタイミングはいくつかオプションがあります。</p>

<p>1つはプロセスが起動したタイミングで全部コンパイルするもの。
コンパイルして実行とあまり変わらないですね。
雑な実装だとたまに見かけます。</p>

<p>1つはメソッド定義時にコンパイルする方法で、上述のSBCLなどいくつか例があります。</p>

<p>似たようなもので、メソッド定義したらタスクをキューに積んで別スレッドのワーカがコンパイルするモデルもTwitterで見たことありますがどの処理系かは覚えてないです。</p>

<p>一番遅いタイミングだとメソッドが呼ばれる直前にコンパイルするものがあります。
呼ばれないメソッドはコンパイルされないというメリットがあります。</p>

<p>いずれの場合もコンパイル時間が実行時間に含まれますのであまり大掛かりな最適化はできません。</p>

<h1 id="jitはどうして速いのか">JITはどうして速いのか</h1>

<p>大体の人は「コンパイルするんだからそりゃ速いだろ」くらいの感覚でしょう。しかしそんなに自明な話ではないです。
インタプリタは大抵Cで書かれてコンパイルされています。これは特にコンパイル時間に制約がないので全力で最適化できます。
時間の制約で適当にしか最適化をできないJITと全力で最適化をしたインタプリタ、本当にJITの方が速いと思いますか？</p>

<p>とは言ったもののインタプリタは遅いです。
なぜなら抽象構文木(AST)を辿りながら実行するからです。
実行のコスト+木の巡回のコストが必要になるのでループなどで何度も同じコードを実行すると木の巡回コストが何倍にもなってのしかかってきます。</p>

<h2 id="vm">VM</h2>

<p>シンプルなインタプリタだと遅いので大抵のインタプリタは一旦(インタプリタよりは)低レベルな命令列にコンパイルし、それを実行するという戦略をとります。
VM型というやつですね。</p>

<p>ちょっと実装してみました。
このVMではこういうことをすると関数が定義できます。
<code>IP_INST_CONST(1)</code> などと並んでいるのがVMの命令ですね。</p>

<pre><code class="language-c">ip_proc_ref_t
ip_register_fib(struct ip_vm *vm)
{


  ip_proc_ref_t fib;

  /* registering proc first to recursive call */
  /* creating en empty proc */


  fib = ip_vm_reserve_proc(vm);
  if (fib &lt; 0) {
    return fib;
  }

  /* 0(arg)   - n */
  size_t nargs = 1;
  size_t nlocals = 0;
  #define n 0
  struct ip_inst body[] = {
                           /*  0 */ IP_INST_CONST(1),
                           /*  1 */ IP_INST_GET_LOCAL(n),
                           /*  2 */ IP_INST_SUB(),
                           /*  3 */ IP_INST_JUMP_IF_NEG(5/* else */),
                           /* then */
                           /*  4 */ IP_INST_CONST(1),
                           /*  5 */ IP_INST_RETURN(),
                           /* else */
                           /*  6 */ IP_INST_GET_LOCAL(n),
                           /*  7 */ IP_INST_CONST(1),
                           /*  8 */ IP_INST_SUB(),
                           /*  9 */ IP_INST_CALL(fib),
                           /* 10 */ IP_INST_GET_LOCAL(n),
                           /* 11 */ IP_INST_CONST(2),
                           /* 12 */ IP_INST_SUB(),
                           /* 13 */ IP_INST_CALL(fib),
                           /* 14 */ IP_INST_ADD(),
                           /* 15 */ IP_INST_RETURN(),
  };

  #undef n

  int ret;
  struct ip_proc *proc;

  ret = ip_proc_new(nargs, nlocals, sizeof(body)/sizeof(body[0]), body, &amp;proc);
  if (ret) {
    return -1;
  }

  ip_vm_register_proc_at(vm, proc, fib);

  return fib;

}
</code></pre>

<p>これを実行するVM部分はこうなります。</p>

<pre><code class="language-c">int
ip_vm_exec(struct ip_vm *vm, ip_proc_ref_t procref)
{
  size_t ip = 0;
  size_t fp;
  struct ip_proc  *proc;

#define LOCAL(i)      ip_stack_ref(ip_value_t, &amp;vm-&gt;stack, fp - (proc-&gt;nargs + proc-&gt;nlocals) + i)
#define POP(ref)      do{if (ip_stack_pop(ip_value_t, &amp;vm-&gt;stack, ref)) { return 1;}} while(0)
#define PUSH(v)       do{if (ip_stack_push(ip_value_t, &amp;vm-&gt;stack, v)) { return 1;}} while(0)
#define POPN(n, ref)  do{size_t i; for (i = 0; i &lt; (n); i++) POP(ref);} while(0)
#define PUSHN(n, v)   do{size_t i; for (i = 0; i &lt; (n); i++) PUSH(v); } while(0)

  proc = vm-&gt;procs[procref];

  PUSHN(proc-&gt;nlocals, IP_LLINT2VALUE(0));
  fp = ip_stack_size(ip_value_t, &amp;vm-&gt;stack);


  while (1) {
    struct ip_inst inst = proc-&gt;insts[ip];
    switch(inst.code) {
    case IP_CODE_CONST: {
      ip_stack_push(ip_value_t, &amp;vm-&gt;stack, inst.u.v);
      break;
    }
    case IP_CODE_GET_LOCAL: {
      int i;
      ip_value_t v;

      i = inst.u.i;
      v = LOCAL(i);

      PUSH(v);
      break;
    }
    case IP_CODE_SET_LOCAL: {
      int i;
      ip_value_t v;

      i = inst.u.i;
      POP(&amp;v);

      LOCAL(i) = v;

      break;
    }
    case IP_CODE_ADD: {
      ip_value_t v1, v2, ret;
      long long int x, y;

      POP(&amp;v1);
      POP(&amp;v2);
      y = IP_VALUE2LLINT(v1);
      x = IP_VALUE2LLINT(v2);

      ret = IP_INT2VALUE(x + y);

      PUSH(ret);

      break;
    }
    case IP_CODE_SUB: {
      ip_value_t v1, v2, ret;
      long long int x, y;

      POP(&amp;v1);
      POP(&amp;v2);
      y = IP_VALUE2LLINT(v1);
      x = IP_VALUE2LLINT(v2);

      ret = IP_LLINT2VALUE(x - y);

      PUSH(ret);

      break;
    }
    case IP_CODE_JUMP: {
      ip = inst.u.pos;
      break;
    }
    case IP_CODE_JUMP_IF_ZERO: {
      ip_value_t v;

      POP(&amp;v);

      if (!IP_VALUE2LLINT(v)) {
        ip = inst.u.pos;
      }
      break;
    }
    case IP_CODE_JUMP_IF_NEG: {
      ip_value_t v;

      POP(&amp;v);

      if (IP_VALUE2LLINT(v) &lt; 0) {
        ip = inst.u.pos;
      }
      break;
    }
    case IP_CODE_CALL: {
      int ret;
      ip_callinfo_t ci = {.ip = ip, .fp = fp, .proc = proc};

      ret = ip_stack_push(ip_callinfo_t, &amp;vm-&gt;callstack, ci);
      if (ret) {
        return 1;
      }

      proc = vm-&gt;procs[inst.u.p];

      PUSHN(proc-&gt;nlocals, IP_LLINT2VALUE(0));

      ip = -1;
      fp = ip_stack_size(ip_value_t, &amp;vm-&gt;stack);


      break;
    }
    case IP_CODE_CALL_INDIRECT: {
      int ret;
      ip_value_t p;
      ip_callinfo_t ci = {.ip = ip, .fp = fp, .proc = proc};

      POP(&amp;p);

      ret = ip_stack_push(ip_callinfo_t, &amp;vm-&gt;callstack, ci);
      if (ret) {
        return 1;
      }

      proc = vm-&gt;procs[IP_VALUE2PROCREF(p)];

      PUSHN(proc-&gt;nlocals, IP_INT2VALUE(0));

      ip = -1;
      fp = ip_stack_size(ip_value_t, &amp;vm-&gt;stack);


      break;
    }
    case IP_CODE_RETURN: {
      int ret;
      ip_value_t v;
      ip_value_t ignore;
      ip_callinfo_t ci;

      POP(&amp;v);

      POPN(proc-&gt;nlocals + proc-&gt;nargs, &amp;ignore);

      PUSH(v);

      ret = ip_stack_pop(ip_callinfo_t, &amp;vm-&gt;callstack, &amp;ci);
      if (ret) {
        return 1;
      }

      ip = ci.ip;
      fp = ci.fp;
      proc = ci.proc;

      break;
    }
    case IP_CODE_EXIT: {
      ip_value_t v;
      ip_value_t ignore;
      POP(&amp;v);

      POPN(proc-&gt;nlocals + proc-&gt;nargs, &amp;ignore);

      PUSH(v);

      return 0;
    }
    default: {
      printf(&quot;code: %d, u: %d&quot;, inst.code, inst.u.i);
      return 1;
    }
    }
    ip += 1;
  }

#undef POP
#undef PUSH
}
</code></pre>

<p>長いですが</p>

<pre><code class="language-c">while(1) {
  struct ip_inst inst = proc-&gt;insts[ip];
  switch(inst.code) {
  case XXX: {
    ...;
    break;
  }
  ...
  }
  ip+=1;
}
</code></pre>

<p>というのが全体の構造です。
配列を舐めているだけなので木を巡回するよりずっと速いです。</p>

<p>ですが、これでもまだ遅いです。必要なコードを実行する以外に<code>while</code> 、 <code>switch</code> 、 <code>break</code> の3回のジャンプが必要になります。
これをどうにか節約できないでしょうか。</p>

<h2 id="threaded-vm">Threaded VM</h2>

<p>どうにか節約するのがThreaded VMです。
<code>break</code> に到達した時点で次に実行する <code>inst</code> は計算できますし <code>inst</code> が分かれば <code>switch</code> でのジャンプ先も分かります。
それに、 <code>switch</code> はただのジャンプテーブルなのでswitchを使わなくてもどうにか自前で実装できます。
なので</p>

<ol>
<li><code>switch</code> の <code>case</code> に相当するジャンプテーブルを自分で作る。</li>
<li><code>break</code> の代わりにそのジャンプテーブルを作って自力で飛ぶ。</li>
</ol>

<p>をやればジャンプ回数を節約できそうです。</p>

<p>これも実装してみました。ただし実装にはGCC拡張の<a href="https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html"><code>Labels as Values</code></a>を使います。</p>

<p>まず、先程の <code>case</code> の部分はラベルになります。</p>

<pre><code class="language-c"> L_CONST: {
    ...
    }
 L_GET_LOCAL: {
    ...
    }
...
</code></pre>

<p>そしてこれを用いてジャンプテーブルは以下のように準備できます。ただのラベルの配列ですね。</p>

<pre><code class="language-c">  static void *labels[] = {
                         &amp;&amp;L_CONST,
                         &amp;&amp;L_GET_LOCAL,
                         &amp;&amp;L_SET_LOCAL,
                         &amp;&amp;L_ADD,
                         &amp;&amp;L_SUB,
                         &amp;&amp;L_JUMP,
                         &amp;&amp;L_JUMP_IF_ZERO,
                         &amp;&amp;L_JUMP_IF_NEG,
                         &amp;&amp;L_CALL,
                         &amp;&amp;L_CALL_INDIRECT,
                         &amp;&amp;L_RETURN,
                         &amp;&amp;L_EXIT,
  };

</code></pre>

<p><code>&amp;&amp;</code> がLabels as Valuesの機能です。ここまでくればこれだけのコードで自前ジャンプができます。</p>

<pre><code class="language-c">inst = proc-&gt;insts[++ip];
goto *labels[inst.code]
</code></pre>

<p>毎度呼ぶのでマクロでまとめるなどして、結局こんな感じのコードになります。</p>

<pre><code class="language-c">#define JUMP()  do{inst = proc-&gt;insts[++ip]; goto *labels[inst.code];} while(0);

  inst = proc-&gt;insts[ip];
  goto *labels[inst.code];

 L_CONST: {
      ip_stack_push(ip_value_t, &amp;vm-&gt;stack, inst.u.v);
      JUMP();
    }
 L_GET_LOCAL: {
      int i;
      ip_value_t v;

      i = inst.u.i;
      v = LOCAL(i);

      PUSH(v);
      JUMP();
    }
...
</code></pre>

<p><code>while</code> と <code>break</code> が消えて <code>switch</code> の代わりに各 <code>case</code> に <code>JUMP</code> が入りました。</p>

<p>これを先程のシンプルなVMと比べてみましょう。ベンチマークに使うのは最初の方に出てきた <code>fib</code> のコードです。引数は36を与えてみます。</p>

<pre><code class="language-console">$ time ./main_simple
result of fib: 24157817
2.45user 0.00system 0:02.45elapsed 99%CPU (0avgtext+0avgdata 1608maxresident)k
$ time ./main_threaded
result of fib: 24157817
1.61user 0.00system 0:01.61elapsed 99%CPU (0avgtext+0avgdata 1536maxresident)k

</code></pre>

<p>$ (2.45 - 1.61) / 2.45 = 0.342&hellip; $ 34%の高速化です。</p>

<h2 id="direct-threaded-vm">Direct Threaded VM</h2>

<p>これでもまだ無駄があります。
先程のコードはジャンプテーブルを使っていました。
しかし次に実行するVM命令は関数を定義した時点で決まっているはずなのでテーブルすら不要です。
要するに <code>inst.code</code> というのがテーブルのインデックスになってましたが、インデックスの代わりに直接ジャンプ先のコードを入れてしまえば速いだろということです。</p>

<p>これも実装してみました。関数定義時には一旦 <code>inst.code</code> を使いますがすぐにアドレスに書換えてしまいます。それがこのコード。</p>

<pre><code class="language-c">for(i = 0; i &lt; ninsts; i++) {
  struct ip_inst_internal inst;
  inst.label = labels[insts[i].code];
  // ...
  result[i] = inst;
}
return 0;
</code></pre>

<p>あとは<code>JUMP</code> を少しだけ書換えたら完了です。</p>

<pre><code class="language-c">// `goto *labels[inst.code]` が `goto *inst.label` になっている
#define JUMP()  do{inst = proc-&gt;insts[++ip]; goto *inst.label;} while(0);
</code></pre>

<p>同じくベンチマークを取ってみましょう。</p>

<pre><code class="language-console">$ time ./main_direct_threaded
result of fib: 24157817
1.52user 0.00system 0:01.52elapsed 99%CPU (0avgtext+0avgdata 1536maxresident)k
</code></pre>

<p>元のVMから比べて $ (2.45 - 1.52) / 2.45 = 0.379&hellip; $ 38%の高速化です。
因みにThreaded VMから比べると $ (1.61 - 1.52) / 1.61 = 0.055&hellip; $ 6%の高速化です。
わずかながら高速化しました。</p>

<p>それでもまだ、ジャンプが1つ残っています。
メソッド定義時点で次に実行するコードは分かっているのでした。
残ったジャンプも取り除けないでしょうか。</p>

<h2 id="なんちゃってjit">なんちゃってJIT</h2>

<p>ジャンプが嫌ならコードをくっつけてしまえばいいのです。
VM命令に対応するネイティブコード片を集めてきて1箇所のメモリに書き込めばジャンプが消えます。</p>

<p>これも実装してみました。</p>

<p>下準備としてVM部分を以下のように書換えます。</p>

<pre><code class="language-c">#define NEXT() arg = proc-&gt;args[++ip];

 L_CONST: {
      ip_stack_push(ip_value_t, &amp;vm-&gt;stack, arg.u.v);
      arg = proc-&gt;args[++ip];
      NEXT();
    }
 L_CONST_END:
 L_GET_LOCAL: {
      int i;
      ip_value_t v;

      i = arg.u.i;
      v = LOCAL(i);

      PUSH(v);
      NEXT();
    }
 L_GET_LOCAL_END:
</code></pre>

<p>ジャンプが消えるので<code>JUMP</code> 内にあったgotoは消えますが、 <code>ip</code> をインクリメントして仮想命令の引数を取り出すところは残ります。
この<code>L_CONST</code> から <code>L_CONST_END</code> までが <code>IP_CODE_CONST</code> に対応するネイティブコードですね。
事前準備としてこれを1命令毎にバッファに書き込んでいきます。</p>

<pre><code class="language-c">case IP_CODE_CONST: {
  code_size = &amp;&amp;L_CONST_END - &amp;&amp;L_CONST
  tmp = realloc(tmp, total_code_size + code_size);
  memcpy(tmp+total_code_size, &amp;&amp;L_CONST, code_size);
  total_code_size += code_size;

}
</code></pre>

<p>他にも多少変更点はあるのですがだいたいこんなところです。
ただしコードを書き込んだメモリは扱いに注意が必要です。
詳しくは2015年のAdvent Calendarに<a href="https://keens.github.io/blog/2015/12/12/sml_dejitwotsukurukaruihanashi/">書きました</a>ので気になる方は一読下さい。</p>

<p>準備が済んだコードの実行は <code>goto</code> するだけです</p>

<pre><code class="language-c">arg = proc-&gt;args[ip];
goto *proc-&gt;code;
</code></pre>

<p>さて、これもベンチマークを取ってみましょう</p>

<pre><code class="language-console">$ time ./main_simple_jit
Command terminated by signal 11
0.00user 0.00system 0:00.10elapsed 0%CPU (0avgtext+0avgdata 1168maxresident)k
</code></pre>

<p>はい、あえなくSEGVしてしまいました。
これは <code>code_size = &amp;&amp;L_CONST_END - &amp;&amp;L_CONST</code> の部分が嘘だったようです。
コンパイルでラベル位置が意図したところに来ないので<code>&amp;&amp;L_CONST_END - &amp;&amp;L_CONST</code>が負になるケースがあるようでした。
これをやりたければアセンブラで頑張るしかないようですね。1日でシンプルなVMからJITまで実装するのは無理だったようです。</p>

<p>動かないのは置いておいて、もし動いたとしたらこれは理想的なコードでしょうか。
これは最適化されたコードをくっつけています。でもやっぱり、くっつけた後に最適化したいですよね？
たとえば以下のような命令列は</p>

<pre><code class="language-c">IP_INST_GET_LOCAL(0),
IP_INST_GET_LOCAL(1),
IP_INST_SUB(),
</code></pre>

<p>コードの並びとしてはこのようになります。</p>

<pre><code class="language-c">{
  ip_value_t v;

  v = LOCAL(0);

  PUSH(v);
}
{
  ip_value_t v;

  v = LOCAL(1);

  PUSH(v);
}
{
  ip_value_t v1, v2, ret;
  long long int x, y;

  POP(&amp;v1);
  POP(&amp;v2);
  y = IP_VALUE2LLINT(v1);
  x = IP_VALUE2LLINT(v2);

  ret = IP_LLINT2VALUE(x - y);

  PUSH(ret);

}
</code></pre>

<p>でも、 <code>PUSH</code> して <code>POP</code> と無駄を挟んでいるので最適化して</p>

<pre><code class="language-c">ip_value_t v1, v2, ret;
long long int x, y;

v1 = LOCAL(0);
v2 = LOCAL(1);

y = IP_VALUE2LLINT(v1);
x = IP_VALUE2LLINT(v2);

ret = IP_LLINT2VALUE(x - y);

PUSH(ret);
</code></pre>

<p>のように短くできるはずです。
これはコード同士の並びがわからないとできないのでくっつけたあとに最適化したいですよね。</p>

<h2 id="コンパイラを使ったjit">コンパイラを使ったJIT</h2>

<p>コードをくっつけたあとにコンパイルする話です。</p>

<p>コードは概ねこのような見た目なのでした。</p>

<pre><code class="language-c">L_CONST: {
    ip_stack_push(ip_value_t, &amp;vm-&gt;stack, arg.u.v);
    arg = proc-&gt;args[++ip];
    NEXT();
  }

</code></pre>

<p>これをマクロにしてみます。</p>

<pre><code class="language-c">#define OP_CONST(v) {                             \
    ip_stack_push(ip_value_t, &amp;vm-&gt;stack, (v));   \
    arg = proc-&gt;args[++ip];                       \
    NEXT();                                       \
  }
</code></pre>

<p>すると、このような命令列から</p>

<pre><code class="language-console">/*  0 */ IP_INST_CONST(1),
/*  1 */ IP_INST_GET_LOCAL(n),
/*  2 */ IP_INST_SUB(),
/*  3 */ IP_INST_JUMP_IF_NEG(5/* else */),
/* then */
/*  4 */ IP_INST_CONST(1),
/*  5 */ IP_INST_RETURN(),
/* else */
/*  6 */ IP_INST_GET_LOCAL(n),
/*  7 */ IP_INST_CONST(1),
/*  8 */ IP_INST_SUB(),
/*  9 */ IP_INST_CALL(fib),
/* 10 */ IP_INST_GET_LOCAL(n),
/* 11 */ IP_INST_CONST(2),
/* 12 */ IP_INST_SUB(),
/* 13 */ IP_INST_CALL(fib),
/* 14 */ IP_INST_ADD(),
/* 15 */ IP_INST_RETURN(),
</code></pre>

<p>以下のようなCのコードを吐くことはたやすいですね。</p>

<pre><code class="language-console">int
run(struct ip_vm *vm)
{
  // 諸々の準備
  OP_CONST(1);
  OP_GET_LOCAL(n);
  OP_SUB();
  OP_JUMP_IF_NEG(L_5);
  OP_CONST(1);
  OP_RETURN();
 L_5:
  OP_GET_LOCAL(n);
  OP_CONST(1);
  OP_SUB();
  OP_CALL(fib);
  OP_GET_LOCAL(n);
  OP_CONST(2);
  OP_SUB();
  OP_CALL(fib);
  OP_ADD();
  OP_RETURN()
  // 諸々の後始末
}
</code></pre>

<p>これをコンパイルしたら望み通りコードをくっつけた後にコンパイルすることができます。目標達成です。</p>

<p>ところ実行中にCコンパイラを呼んで新しい実行可能ファイルを作ったところでどう実行するんだと思うかもしれません。
これはDLLを作ってロードしてあげれば実行できます。
そういう話は話はやはり2015年のAdvent Calendarに<a href="https://keens.github.io/blog/2015/12/12/sml_nimanabukonpairagengoniokerureplnojissouhouhou/">書いた</a>ので参考にして下さい。</p>

<p>これも実装してみようとしましたが先のJITの例が動かなかったので萎えて書いてないです。
ちゃんと実装したら多分動きます。</p>

<h2 id="もうちょっと">もうちょっと</h2>

<p>最終的にCのコードを吐くところまでいきました。実行性能は申し分無いはずです。これより先はあるでしょうか。</p>

<p>やっぱり、実行中にCコンパイラのプロセスを呼ぶのは筋が悪いです。
外部システムに依存しますしプロセスの起動はそこそこ重い処理です。
最適化もCのセマンティクスを通すので本来処理系が知ってるはずの情報が落ちたりもします。
さらにDLLにしてロードというだけで時間的にも空間的にもオーバーヘッドがあります。
そこを気にしだすとインメモリでアセンブルして自前でコード生成になったりします。</p>

<p>ならば、とLLVMを使う例もありますがLLVMはコンパイラ向けの中間言語を使うので軽い気持ちで使うと思ったより面倒なことになるでしょう。
LLVMを使ってJITをしていたRubyのRubiniusやPythonのPystonが奮わないのもそのせいかもしれません。あとLLVMつらいらしい(あんまりLLVM使ったことがないので伝聞)。</p>

<p>あるいはアセンブラを直接書けるとその処理系に特化したコード生成できたりします。
Common LispだとCのコードを吐いてコンパイルする処理系にECLというのがありますが、ほとんどのベンチマークで、自分でアセンブラまで持ってるSBCLより遅いです。
これはCコンパイラが弱い訳ではなくてSBCLがかなり工夫を凝らした実装をしているからです。
たとえば多値と単値を区別するのにキャリーフラグを使っておりレジスタ使用量を抑えたりしています。この話題は最近<a href="https://keens.github.io/slide/common_lispnotachitosonojissoutachi/">話した</a>ので良かったら見てみて下さい。</p>

<p>歴史あるCommon Lisp処理系のSBCLは様々なプラットホーム向けに頑張ってアセンブラを実装していますが流石にそこまでやれる処理系はそう多くないでしょう。ライブラリの助けを借りることになります。
JITライブラリもいくつかあります。LibJITやGNU Lightning、もう少し軽いのならXbyakなど。あとLuaJITのやつなんだっけ。</p>

<p>ツールは主眼であった問題を解決してくれますが、今度はツールを上手く使いこなすという問題がでます。
たとえば命令の並びが決定すると <code>PUSH</code> と <code>POP</code> が減らせるという話もメモリが絡む最適化はCコンパイラが苦手とするところなのでやってくれないかもしれません。
結局は作者側での工夫が必要でしょう。</p>

<h1 id="jitを超えた先へ">JITを超えた先へ</h1>

<p>JITの常識を超えてみましょう。何があるでしょうか。</p>

<p>たとえば、2度コンパイルする。</p>

<p>たとえば、コンパイルしない。</p>

<h2 id="適応的最適化">適応的最適化</h2>

<p>折角ランタイムにコード生成しているのでもうちょっとダイナミックなことをしてみましょう。</p>

<p>ある関数が何度も呼ばれることが分かった場合、時間を掛けてでも最適化する価値があります。
たとえばHTTPサーバのハンドラは何日間ものあいだ無数に実行されるのでコンパイルに1分掛かろうがペイするでしょう。
何度も呼ばれる関数はもう一度、最適化レベルを上げてコンパイルし直すというのが考えられます。</p>

<h2 id="適応的コンパイル">適応的コンパイル</h2>

<p>先程までは全てのコードを最適な状態で動かす前提で話していました。
しかしDirect Threaded VMあたりから事前準備が必要になりました。
これは各々の関数定義にオーバーヘッドを加えるのであまり実行させない関数に対して行うとかえって遅くなったりします。
そこでまずはオーバーヘッドのかからない実行方式で実行し、それなりに実行される関数と判明したらもうちょっと速く実行できる方式に切り替えるというのが考えられます。
つまり、必要なければコンパイルしない。</p>

<p>この方法はコンパイルが間に合ってないのでJust in Timeではなないですね。
なので厳密にいうとJITには含まれませんがこういうのも含めてJITと呼ぶことが多いようです。</p>

<p>一見良さそうなアイディアですが今までの方式と比べて大きな技術的トレードオフがあります。
VMとコンパイルされるコード、2種類のインタプリタを実装する必要があるのです。
これは手間なだけではなく両者で挙動が違うと、たとえば256回実行すると結果が変わるバグなどになります。
この大きなトレードオフにも関わらず適応的コンパイルを選択するケースが多いようです(要出典)。みなさん頑張りやさんですね。</p>

<h2 id="tracing-jit">Tracing JIT</h2>

<p>今まで、関数(メソッド)単位でコンパイルする話をしてきました。別の方法がないか考えてみましょう。</p>

<p>たとえばこのような命令列を考えます</p>

<pre><code class="language-console">/*  0 */ IP_INST_CONST(0),
/*  1 */ IP_INST_SET_LOCAL(i),
/*  2 */ IP_INST_CONST(0),
/*  3 */ IP_INST_SET_LOCAL(sum),
/* loop */
/*  4 */ IP_INST_GET_LOCAL(n),
/*  5 */ IP_INST_GET_LOCAL(i),
/*  6 */ IP_INST_SUB(),
/*  7 */ IP_INST_JUMP_IF_NEG(16 /* exit */),
/*  8 */ IP_INST_GET_LOCAL(sum),
/*  9 */ IP_INST_GET_LOCAL(i),
/* 10 */ IP_INST_ADD(),
/* 11 */ IP_INST_SET_LOCAL(sum),
/* 12 */ IP_INST_GET_LOCAL(i),
/* 13 */ IP_INST_CONST(1),
/* 14 */ IP_INST_ADD(),
/* 15 */ IP_INST_SET_LOCAL(i),
/* 16 */ IP_INST_JUMP(3 /* loop */),
/* exit */
/* 17 */ IP_INST_GET_LOCAL(sum),
/* 18 */ IP_INST_RETURN(),
</code></pre>

<p>これを実行するともちろんループ内のコードが繰り返し実行されますね。
もし実行されるVM命令列のトレースを取ったらこのような見た目になるでしょう。</p>

<pre><code class="language-console">GET_LOCAL(0)
GET_LOCAL(1)
SUB()
JUMP_IF_NEG(16)
GET_LOCAL(2)
GET_LOCAL(1)
ADD()
SET_LOCAL(2)
GET_LOCAL(1)
CONST(1)
ADD()
SET_LOCAL(1)
JUMP(3)
/* 2周目 */
GET_LOCAL(0)
GET_LOCAL(1)
SUB()
JUMP_IF_NEG(16)
GET_LOCAL(2)
GET_LOCAL(1)
ADD()
SET_LOCAL(2)
GET_LOCAL(1)
CONST(1)
ADD()
SET_LOCAL(1)
JUMP(3)
...
</code></pre>

<p>これは簡単な例でしたがループ内で関数呼び出しがあったり分岐があったりするともう少し状況が複雑になります。
それでも、「よく実行されるトレースパターン」というのが浮かび上がるはずです。
このトレースベースでコンパイルしよう、というのがトレーシングJITです。</p>

<p>私はTracing JITについてPyPyの人のブログを読んで感動した記憶があります。
2012年の記事ですがサンプル実装コードがPrologだったりかなりのニンジャであることが伺える楽しい連載なので是非一読下さい。</p>

<p><a href="https://morepypy.blogspot.com/2012/01/comparing-partial-evaluation-and.html">Comparing Partial Evaluation and Tracing, Part 1 </a><br />
素晴らしいことに和訳もあります<br />
<a href="http://morepypy-ja.blogspot.com/2012/01/part-1.html">部分評価とトレーシングの比較 Part 1 </a><br />
つながりが見づらいですが左のアーカイブから2012年の1月と2月の記事を開くと続きが見つかります。</p>

<p>さて、Tracing JITですがこれはとてつもなく強力です。
関数に縛られないので数度しか呼ばれないけどものすごく重いループを回す関数もループ内だけ最適化できます。
関数を跨いでのトレースを取れば関数を跨いでの最適化ができます。
ifのthen節がよく実行されるならthen節のみのトレースが出てくるのでifの条件節が常にtrueであることを前提に最適化できます。
たとえば <code>if x isinstanceof Int</code> がtrueならば以後は動的型チェックを省いて実行できたりします。</p>

<p>しかしやはりこれにもトレードオフが入ります。これも適応的コンパイルの一種なので適応的コンパイルのデメリットはそのまま残りますが、さらなるデメリットがあります。
メソッドの実行途中でVMから抜けてネイティブコードになったりネイティブコードを実行してたら急にVMに戻ったりするわけです。
そういう橋渡しがかなり難しいと聞きます。
が、私は詳しくないので詳しい方に説明をゆずります。言語実装 Advent Calendarの続きにご期待下さい。</p>

<h1 id="最後に">最後に</h1>

<p>遅刻して大変申し訳ありませんでした。敗因は「VMくらい2時間程度あれば書けるだろ」とタカをくくってたらDirect Threaded VMとJITのデバッグでかなり時間を取られたことです。あと数年ぶりにCを書いた。
因みにVM自体はWebassemblyやHuman Resource Machineを真似して命令セットを書いたら書きやすかったです。</p>

<p>「JITを超えた先へ」のところはぶっちゃけ全然知らないです。PyPyのブログを読むまでは適応的コンパイルのことをTracing JITと思ってました。</p>

<h1 id="ノート">ノート</h1>

<ul>
<li>今回書いたコードは<a href="https://github.com/KeenS/interp">こちら</a>です</li>
<li>VM, Threaded VM, Direct Threaded VMに関しては毎度ながらこの記事を参考にさせていただきました。
<a href="https://magazine.rubyist.net/articles/0008/0008-YarvManiacs.html">YARV Maniacs 【第 3 回】 命令ディスパッチの高速化</a></li>
<li>今回私が書いたVMはメモリの扱いがないので比較的 1命令あたりの計算量が少ないです。実際の処理系だと少し結果が変わるかもしれません</li>
<li>とはいえ過去に数十%の高速化に成功したこともあるので場合によりけり。 CF <a href="https://keens.github.io/blog/2015/05/26/onigmowosaidai49_kousokukashitahanashi/">Onigmoを最大49%高速化した話</a>

<ul>
<li>余談ですが当時Threaded VMとDirect Threaded VMの違いを分かってなかったのでThreaded VMなのにDirect Threaded VMと呼んでしまってます。</li>
</ul></li>
<li>ラベルとラベルの間のコードをコピーするJITの話はOracle Labのどこかのスライドで見た記憶から実装しましたが思い出せませんでした。</li>
<li>マクロを定義してCコードを生成するのは確かmoclで見た気がします。恐らくKCLの頃からやってるんじゃないでしょうか。</li>
<li>JITにCコンパイラを使うと無駄が多いという話をしましたがRubyのJITは色々工夫を凝らしているようです。言語実装 Advent Calendarの最終日にその話があるようなので期待しましょう。</li>
</ul>

<h1 id="付録">付録</h1>

<p><blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">JITあれこれ <a href="https://t.co/pwdpM5jzVA">https://t.co/pwdpM5jzVA</a> 分かりやすくて丁寧なJITの解説。私が試した時は、Threaded VMはそれほど速くならなかった気がするので、使ったコンパイラとOSを書いてくれるとより良いかなと思った</p>&mdash; Miura Hideki (@miura1729) <a href="https://twitter.com/miura1729/status/1069053214190841856?ref_src=twsrc%5Etfw">2018年12月2日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>確かにそうですね。本文中でベンチマークを取った環境は以下です。</p>

<ul>
<li>コンパイラ: gcc (Ubuntu 8.2.0-7ubuntu1) 8.2.0</li>
<li>OS: Ubuntu 18.10</li>
<li>CPU: AMD Ryzen Threadripper 1950X 16-Core Processor</li>
</ul>

<p>AMDマシンですね。</p>

<p>ところで私はもう一つマシンを持ってるのでこちらでもベンチマークをしてみます。</p>

<ul>
<li>コンパイラ: gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0</li>
<li>OS: Ubuntu 18.04</li>
<li>CPU: Intel&reg; Core&trade; i7-4910MQ CPU @ 2.90GHz</li>
</ul>

<p>こちらはIntelマシンです。ベンチマークの結果はこうなりました。</p>

<pre><code class="language-console">$ time ./main_simple
result of fib: 24157817
1.23user 0.00system 0:01.24elapsed 99%CPU (0avgtext+0avgdata 1556maxresident)k
$ time ./main_threaded
result of fib: 24157817
1.24user 0.00system 0:01.24elapsed 100%CPU (0avgtext+0avgdata 1552maxresident)k
$ time ./main_direct_threaded
result of fib: 24157817
1.19user 0.00system 0:01.19elapsed 100%CPU (0avgtext+0avgdata 1532maxresident)k
</code></pre>

<p>うおー、Threaded VMで遅くなってる。
コンパイラのバージョンとCPUの両方が変わってるのでわかりませんがIntel CPUの方が分岐予測が賢いかGCCのバージョンが上がって <code>while (){switch (){}}</code> のコンパイルが賢くなったか、あるいはそのどちらともでしょう。</p>

<p>ところで、上記までのベンチマークは最適化オプションを <code>-O3</code> にしていました。アーキテクチャ間の差異が気になったので <code>-march=native</code> を付けて使っているCPU向けに最適化してみます。</p>

<p>AMDマシン</p>

<pre><code class="language-console">$ time ./main_simple
result of fib: 24157817
1.66user 0.00system 0:01.66elapsed 99%CPU (0avgtext+0avgdata 1612maxresident)k
0inputs+0outputs (0major+68minor)pagefaults 0swaps
$ time ./main_threaded
result of fib: 24157817
1.60user 0.00system 0:01.61elapsed 99%CPU (0avgtext+0avgdata 1548maxresident)k
0inputs+0outputs (0major+67minor)pagefaults 0swaps
$ time ./main_direct_threaded
result of fib: 24157817
1.50user 0.00system 0:01.50elapsed 99%CPU (0avgtext+0avgdata 1536maxresident)k
0inputs+0outputs (0major+66minor)pagefaults 0swaps
</code></pre>

<p>Intelマシン</p>

<pre><code class="language-c">$ time ./main_simple
result of fib: 24157817
1.21user 0.00system 0:01.21elapsed 100%CPU (0avgtext+0avgdata 1488maxresident)k
$ time ./main_threaded
result of fib: 24157817
1.23user 0.00system 0:01.23elapsed 99%CPU (0avgtext+0avgdata 1560maxresident)k
$ time ./main_direct_threaded
result of fib: 24157817
1.22user 0.00system 0:01.22elapsed 99%CPU (0avgtext+0avgdata 1508maxresident)k
</code></pre>

<p>まとめてみます</p>

<table>
<thead>
<tr>
<th align="left">.</th>
<th align="right">Simple VM</th>
<th align="right">Threaded VM</th>
<th align="right">vs Simple</th>
<th align="right">Direct Threaded VM</th>
<th align="right">vs Simple</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">マシン1/<code>-O3</code></td>
<td align="right">2.45</td>
<td align="right">1.61</td>
<td align="right">34%</td>
<td align="right">1.52</td>
<td align="right">38%</td>
</tr>

<tr>
<td align="left">マシン1/<code>-O3 -march=native</code></td>
<td align="right">1.66</td>
<td align="right">1.60</td>
<td align="right">3.6%</td>
<td align="right">1.50</td>
<td align="right">10%</td>
</tr>

<tr>
<td align="left">マシン2/<code>-O3</code></td>
<td align="right">1.23</td>
<td align="right">1.24</td>
<td align="right">-0.8%</td>
<td align="right">1.19</td>
<td align="right">3.2%</td>
</tr>

<tr>
<td align="left">マシン2/<code>-O3 -march=native</code></td>
<td align="right">1.21</td>
<td align="right">1.23</td>
<td align="right">-1.7%</td>
<td align="right">1.22</td>
<td align="right">-0.8%</td>
</tr>
</tbody>
</table>

<p>マシン1 = AMD/Ubuntu 18.10/gcc 8.2.0<br />
マシン2 = Intel/Ubuntu 18.04/gcc 7.3.0</p>

<p>参考になれば。</p>
                    </section>
                </article>
            </div>
        </div>

        <div class="row li-author">
    <div class="sixteen columns">
        Written by <strong>κeen</strong>
    </div>
</div>


        <div class="row li-pagination">
            <div class="eight columns">
                <div class="li-pagination-previous">
                    
                        Later article<br />
                        <a href="//KeenS.github.io/blog/2018/12/08/rustnomoju_runotsukaikata_2018_editionhan/"> Rustのモジュールの使い方 2018 Edition版</a>
                    
                </div>
            </div>
            <div class="eight columns">
                <div class="li-pagination-next">
                    
                        Older article<br />
                        <a href="//KeenS.github.io/blog/2018/11/17/gengoshorikeibenkyoukainisankashitekita/"> 言語処理系勉強会に参加してきた</a>
                    
                </div>
            </div>
        </div>
    </div>

<footer class="li-page-footer">
    <div class="container">
        <div class="row">
            <div class="sixteen columns">
                <div class="li-page-footer-legal">
                    &copy; 2019. All rights reserved. 
                </div>
                <div class="li-page-footer-theme">
                    <span class=""><a href="https://github.com/eliasson/liquorice/">liquorice</a> is a theme for <a href="http://hugo.spf13.com">hugo</a></span>
                </div>
            </div>
        </div>
    </div>
</footer>

    <script type="text/javascript">
    <!--
    function toggle(id) {
        var e = document.getElementById(id);
        e.style.display == 'block' ? e.style.display = 'none' : e.style.display = 'block';
    }
    
    </script>
    <script type="text/javascript">
     <!--
                                   var _gaq = _gaq || [];
     _gaq.push(['_setAccount', ""]);
     _gaq.push(['_trackPageview']);

     (function() {
         var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
         ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
         var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
     })();
    
    </script>
    <script src="https://unpkg.com/mermaid@7.0.4/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true
     });</script>

    </body>
</html>

